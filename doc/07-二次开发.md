# ğŸ› ï¸ äºŒæ¬¡å¼€å‘

æœ¬æ–‡æ¡£æä¾› MDtranslator çš„æ‰©å±•å¼€å‘æŒ‡å—ï¼Œå¸®åŠ©ä½ å®šåˆ¶å’Œæ‰©å±•åŠŸèƒ½ã€‚

## ç›®å½•

- [æ‰©å±•ç‚¹æ¦‚è§ˆ](#æ‰©å±•ç‚¹æ¦‚è§ˆ)
- [è‡ªå®šä¹‰ LLM æä¾›å•†](#è‡ªå®šä¹‰-llm-æä¾›å•†)
- [è‡ªå®šä¹‰æç¤ºè¯](#è‡ªå®šä¹‰æç¤ºè¯)
- [æ·»åŠ æ–°åŠŸèƒ½](#æ·»åŠ æ–°åŠŸèƒ½)
- [ä¿®æ”¹åˆ†å—ç­–ç•¥](#ä¿®æ”¹åˆ†å—ç­–ç•¥)
- [æ‰©å±•å­˜å‚¨åç«¯](#æ‰©å±•å­˜å‚¨åç«¯)
- [è‡ªå®šä¹‰ UI ç»„ä»¶](#è‡ªå®šä¹‰-ui-ç»„ä»¶)
- [å¼€å‘æœ€ä½³å®è·µ](#å¼€å‘æœ€ä½³å®è·µ)

---

## æ‰©å±•ç‚¹æ¦‚è§ˆ

MDtranslator æä¾›å¤šä¸ªæ‰©å±•ç‚¹ï¼Œæ–¹ä¾¿äºŒæ¬¡å¼€å‘ï¼š

| æ‰©å±•ç‚¹ | ä½ç½® | è¯´æ˜ |
|:---|:---|:---|
| LLM æä¾›å•† | `backend/routers/translate.py` | æ·»åŠ æ–°çš„ç¿»è¯‘å¼•æ“ |
| æç¤ºè¯æ¨¡æ¿ | `backend/prompts/` | è‡ªå®šä¹‰ç¿»è¯‘æç¤ºè¯ |
| åˆ†å—ç­–ç•¥ | `backend/markdown_utils.py` | ä¿®æ”¹æ–‡æ¡£åˆ†å‰²é€»è¾‘ |
| å­˜å‚¨åç«¯ | `backend/persistent_storage.py` | æ›¿æ¢ä¸ºå…¶ä»–æ•°æ®åº“ |
| UI ç»„ä»¶ | `src/src/components/` | è‡ªå®šä¹‰ç•Œé¢ç»„ä»¶ |
| çŠ¶æ€ç®¡ç† | `src/src/store/` | æ‰©å±•åº”ç”¨çŠ¶æ€ |
| API è·¯ç”± | `backend/routers/` | æ·»åŠ æ–°çš„ API ç«¯ç‚¹ |

---

## è‡ªå®šä¹‰ LLM æä¾›å•†

### æ·»åŠ  OpenAI æ”¯æŒ

é»˜è®¤ä½¿ç”¨é€šä¹‰åƒé—®ï¼Œå¯ä»¥è½»æ¾åˆ‡æ¢åˆ° OpenAIï¼š

```python
# backend/routers/translate.py

def get_llm_client():
    """æ ¹æ®é…ç½®åˆ›å»º LLM å®¢æˆ·ç«¯"""
    provider = os.getenv("LLM_PROVIDER", "qwen")
    
    if provider == "openai":
        return AsyncOpenAI(
            api_key=os.getenv("OPENAI_API_KEY"),
            # ä½¿ç”¨é»˜è®¤ base_url
        )
    elif provider == "azure":
        return AsyncOpenAI(
            api_key=os.getenv("AZURE_API_KEY"),
            base_url=os.getenv("AZURE_ENDPOINT"),
            default_headers={"api-version": "2024-02-01"}
        )
    else:  # qwen (default)
        return AsyncOpenAI(
            api_key=os.getenv("QWEN_API_KEY"),
            base_url=os.getenv("QWEN_API_URL", 
                "https://dashscope.aliyuncs.com/compatible-mode/v1")
        )
```

### ç¯å¢ƒå˜é‡é…ç½®

```env
# .env

# åˆ‡æ¢æä¾›å•†
LLM_PROVIDER=openai  # æˆ– qwen, azure

# OpenAI é…ç½®
OPENAI_API_KEY=sk-xxx
OPENAI_MODEL=gpt-4o

# Azure OpenAI é…ç½®
AZURE_API_KEY=xxx
AZURE_ENDPOINT=https://your-resource.openai.azure.com
AZURE_MODEL=gpt-4

# é€šä¹‰åƒé—®é…ç½® (é»˜è®¤)
QWEN_API_KEY=xxx
QWEN_MODEL_NAME=qwen-flash
```

### æ·»åŠ æœ¬åœ°æ¨¡å‹æ”¯æŒ (Ollama)

```python
# æ·»åŠ  Ollama æ”¯æŒ
elif provider == "ollama":
    return AsyncOpenAI(
        api_key="ollama",  # Ollama ä¸éœ€è¦çœŸå® key
        base_url="http://localhost:11434/v1"
    )
```

---

## è‡ªå®šä¹‰æç¤ºè¯

### æç¤ºè¯æ–‡ä»¶ä½ç½®

```
backend/prompts/system_prompt.txt
```

### é»˜è®¤æç¤ºè¯ç»“æ„

```text
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ€æœ¯æ–‡æ¡£ç¿»è¯‘ä¸“å®¶ã€‚è¯·å°†ä»¥ä¸‹ Markdown å†…å®¹ç¿»è¯‘æˆä¸­æ–‡ã€‚

è¦æ±‚ï¼š
1. ä¿æŒåŸæœ‰çš„ Markdown æ ¼å¼ä¸å˜
2. ä»£ç å—ã€ä»£ç å†…å®¹ä¸ç¿»è¯‘
3. ä¸“æœ‰åè¯ä¿æŒåŸæ–‡
4. ç¿»è¯‘å‡†ç¡®ã€é€šé¡º
5. ä¿æŒåŸæ–‡çš„è¯­æ°”å’Œé£æ ¼
```

### è‡ªå®šä¹‰ç›®æ ‡è¯­è¨€

ä¿®æ”¹æç¤ºè¯æ”¯æŒå…¶ä»–è¯­è¨€ï¼š

```text
# backend/prompts/system_prompt_ja.txt

ã‚ãªãŸã¯ãƒ—ãƒ­ã®æŠ€è¡“æ–‡æ›¸ç¿»è¨³è€…ã§ã™ã€‚
ä»¥ä¸‹ã® Markdown ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚

è¦ä»¶ï¼š
1. å…ƒã® Markdown å½¢å¼ã‚’ç¶­æŒ
2. ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã¯ç¿»è¨³ã—ãªã„
3. å›ºæœ‰åè©ã¯åŸæ–‡ã‚’ç¶­æŒ
...
```

### åŠ¨æ€æç¤ºè¯é€‰æ‹©

```python
# backend/routers/translate.py

def load_system_prompt(target_lang: str = "zh"):
    """æ ¹æ®ç›®æ ‡è¯­è¨€åŠ è½½æç¤ºè¯"""
    prompt_file = f"system_prompt_{target_lang}.txt"
    prompt_path = os.path.join(
        os.path.dirname(__file__), "..", "prompts", prompt_file
    )
    
    if not os.path.exists(prompt_path):
        prompt_path = prompt_path.replace(f"_{target_lang}", "")
    
    with open(prompt_path, "r", encoding="utf-8") as f:
        return f.read()
```

---

## æ·»åŠ æ–°åŠŸèƒ½

### ç¤ºä¾‹ï¼šæ·»åŠ æœ¯è¯­è¡¨åŠŸèƒ½

**1. æ·»åŠ æ•°æ®åº“è¡¨**

```python
# backend/persistent_storage.py

CREATE_GLOSSARY_TABLE = """
CREATE TABLE IF NOT EXISTS glossary (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    source_term TEXT NOT NULL,
    target_term TEXT NOT NULL,
    category TEXT DEFAULT 'general',
    created_at TEXT
)
"""

class PersistentStore:
    async def _ensure_initialized(self, conn):
        # ... ç°æœ‰ä»£ç  ...
        await conn.execute(CREATE_GLOSSARY_TABLE)
    
    async def add_glossary_term(self, source: str, target: str, category: str = "general"):
        """æ·»åŠ æœ¯è¯­"""
        async with self._get_connection() as conn:
            await conn.execute(
                "INSERT INTO glossary (source_term, target_term, category, created_at) VALUES (?, ?, ?, ?)",
                (source, target, category, datetime.now().isoformat())
            )
            await conn.commit()
    
    async def get_glossary(self, category: str = None) -> List[Dict]:
        """è·å–æœ¯è¯­è¡¨"""
        async with self._get_connection() as conn:
            if category:
                cursor = await conn.execute(
                    "SELECT * FROM glossary WHERE category = ?", (category,)
                )
            else:
                cursor = await conn.execute("SELECT * FROM glossary")
            return [dict(row) for row in await cursor.fetchall()]
```

**2. æ·»åŠ  API è·¯ç”±**

```python
# backend/routers/glossary.py

from fastapi import APIRouter
from persistent_storage import store

router = APIRouter()

@router.get("/api/glossary")
async def get_glossary(category: str = None):
    terms = await store.get_glossary(category)
    return {"terms": terms}

@router.post("/api/glossary")
async def add_glossary_term(request: GlossaryRequest):
    await store.add_glossary_term(
        request.source_term, 
        request.target_term,
        request.category
    )
    return {"success": True}
```

**3. åœ¨ç¿»è¯‘ä¸­ä½¿ç”¨æœ¯è¯­è¡¨**

```python
# backend/routers/translate.py

async def translate_chunk_task(...):
    # åŠ è½½æœ¯è¯­è¡¨
    glossary = await document_store.get_glossary()
    glossary_text = "\n".join(
        f"- {t['source_term']} â†’ {t['target_term']}" 
        for t in glossary
    )
    
    # æ·»åŠ åˆ°æç¤ºè¯
    system_content = SYSTEM_PROMPT + f"\n\næœ¯è¯­è¡¨ï¼š\n{glossary_text}"
```

---

## ä¿®æ”¹åˆ†å—ç­–ç•¥

### å½“å‰ç­–ç•¥

åŸºäº H1/H2 æ ‡é¢˜è¾¹ç•Œåˆ†å‰²ï¼Œä¿æŒæ–‡æ¡£ç»“æ„å®Œæ•´ã€‚

### è‡ªå®šä¹‰åˆ†å—

```python
# backend/markdown_utils.py

def split_into_chunks_by_tokens(content: str, max_tokens: int = 1000) -> List[Dict]:
    """æŒ‰ token æ•°é‡åˆ†å—ï¼ˆé€‚åˆé•¿æ–‡æ¡£ï¼‰"""
    import tiktoken
    
    encoding = tiktoken.encoding_for_model("gpt-4")
    tokens = encoding.encode(content)
    
    chunks = []
    current_start = 0
    
    while current_start < len(tokens):
        end = min(current_start + max_tokens, len(tokens))
        chunk_tokens = tokens[current_start:end]
        chunk_text = encoding.decode(chunk_tokens)
        
        chunks.append({
            "chunk_index": len(chunks),
            "raw_text": chunk_text,
            "translated_text": None,
            "status": "pending"
        })
        
        current_start = end
    
    return chunks


def split_into_chunks_by_paragraph(content: str, min_paragraphs: int = 3) -> List[Dict]:
    """æŒ‰æ®µè½åˆ†å—ï¼ˆé€‚åˆçŸ­æ–‡æ¡£ï¼‰"""
    paragraphs = content.split('\n\n')
    
    chunks = []
    current_chunk = []
    
    for para in paragraphs:
        current_chunk.append(para)
        
        if len(current_chunk) >= min_paragraphs:
            chunks.append({
                "chunk_index": len(chunks),
                "raw_text": '\n\n'.join(current_chunk),
                "translated_text": None,
                "status": "pending"
            })
            current_chunk = []
    
    # å¤„ç†å‰©ä½™æ®µè½
    if current_chunk:
        chunks.append({
            "chunk_index": len(chunks),
            "raw_text": '\n\n'.join(current_chunk),
            "translated_text": None,
            "status": "pending"
        })
    
    return chunks
```

---

## æ‰©å±•å­˜å‚¨åç«¯

### æ›¿æ¢ä¸º PostgreSQL

```python
# backend/persistent_storage_postgres.py

import asyncpg
from typing import Dict, List, Optional

class PostgresStore:
    def __init__(self, connection_string: str):
        self.connection_string = connection_string
        self._pool = None
    
    async def initialize(self):
        self._pool = await asyncpg.create_pool(self.connection_string)
        async with self._pool.acquire() as conn:
            await conn.execute("""
                CREATE TABLE IF NOT EXISTS documents (
                    id UUID PRIMARY KEY,
                    title TEXT,
                    original_content TEXT,
                    translated_content TEXT DEFAULT '',
                    chunks_data JSONB DEFAULT '[]',
                    status TEXT DEFAULT 'pending',
                    created_at TIMESTAMPTZ DEFAULT NOW(),
                    updated_at TIMESTAMPTZ DEFAULT NOW()
                )
            """)
    
    async def create_document(self, doc_id: str, title: str, 
                              original_content: str, chunks_data: list) -> Dict:
        async with self._pool.acquire() as conn:
            await conn.execute(
                """INSERT INTO documents (id, title, original_content, chunks_data)
                   VALUES ($1, $2, $3, $4)""",
                doc_id, title, original_content, json.dumps(chunks_data)
            )
        return { ... }
```

### ä½¿ç”¨ Redis ç¼“å­˜

```python
# backend/cache.py

import redis.asyncio as redis
import json

class RedisCache:
    def __init__(self, url: str = "redis://localhost"):
        self.redis = redis.from_url(url)
    
    async def get_document(self, doc_id: str):
        data = await self.redis.get(f"doc:{doc_id}")
        return json.loads(data) if data else None
    
    async def cache_document(self, doc_id: str, doc: dict, ttl: int = 3600):
        await self.redis.setex(
            f"doc:{doc_id}",
            ttl,
            json.dumps(doc)
        )
```

---

## è‡ªå®šä¹‰ UI ç»„ä»¶

### æ·»åŠ æ–°ä¸»é¢˜

```css
/* src/src/app/globals.css */

/* æš—è‰²ä¸»é¢˜ */
.dark-theme {
  --bg-primary: #1a1a2e;
  --bg-secondary: #16213e;
  --text-primary: #eee;
  --text-secondary: #a0a0a0;
  --accent: #e94560;
}

.dark-theme .markdown-preview {
  background: var(--bg-primary);
  color: var(--text-primary);
}

.dark-theme .markdown-preview code {
  background: var(--bg-secondary);
}
```

### æ·»åŠ ä¸»é¢˜åˆ‡æ¢

```tsx
// src/src/components/ThemeToggle.tsx

import { useState, useEffect } from 'react';

export function ThemeToggle() {
  const [isDark, setIsDark] = useState(false);
  
  useEffect(() => {
    document.body.classList.toggle('dark-theme', isDark);
  }, [isDark]);
  
  return (
    <button onClick={() => setIsDark(!isDark)}>
      {isDark ? 'ğŸŒ™' : 'â˜€ï¸'}
    </button>
  );
}
```

### è‡ªå®šä¹‰ç¼–è¾‘å™¨ä¸»é¢˜

```tsx
// src/src/components/Editor.tsx

import { oneDark } from '@codemirror/theme-one-dark';

export default function Editor({ value, onChange, theme = 'light' }) {
  return (
    <CodeMirror
      value={value}
      theme={theme === 'dark' ? oneDark : undefined}
      extensions={[markdown({ ... })]}
      onChange={onChange}
    />
  );
}
```

---

## å¼€å‘æœ€ä½³å®è·µ

### ä»£ç è§„èŒƒ

```
âœ… ä½¿ç”¨ TypeScript ç±»å‹æ³¨è§£
âœ… ä½¿ç”¨ async/await è€Œéå›è°ƒ
âœ… ç»„ä»¶ä½¿ç”¨å‡½æ•°å¼å†™æ³•
âœ… çŠ¶æ€ç®¡ç†ä½¿ç”¨ Zustand
âœ… åç«¯ä½¿ç”¨ç»å¯¹å¯¼å…¥
âœ… æäº¤å‰è¿è¡Œ lint æ£€æŸ¥
```

### ç›®å½•ç»“æ„è§„èŒƒ

```
æ–°åŠŸèƒ½åº”è¯¥ï¼š
1. åç«¯è·¯ç”±æ”¾åœ¨ backend/routers/
2. å‰ç«¯ç»„ä»¶æ”¾åœ¨ src/src/components/
3. è‡ªå®šä¹‰ hooks æ”¾åœ¨ src/src/hooks/
4. ç±»å‹å®šä¹‰é›†ä¸­ç®¡ç†
5. å·¥å…·å‡½æ•°æ”¾åœ¨ lib/ æˆ– utils/
```

### æµ‹è¯•å»ºè®®

```bash
# åç«¯æµ‹è¯•
cd backend
pytest tests/

# å‰ç«¯æµ‹è¯•
cd src
npm test
```

### Git æäº¤è§„èŒƒ

```
feat: æ–°åŠŸèƒ½
fix: ä¿®å¤ bug
docs: æ–‡æ¡£æ›´æ–°
style: ä»£ç æ ¼å¼
refactor: é‡æ„
test: æµ‹è¯•ç›¸å…³
chore: æ„å»º/å·¥å…·
```

---

## ç¤ºä¾‹é¡¹ç›®

### æ·»åŠ æ‰¹é‡ç¿»è¯‘åŠŸèƒ½

å®Œæ•´ç¤ºä¾‹ï¼šæ”¯æŒä¸€æ¬¡ç¿»è¯‘å¤šä¸ªæ–‡ä»¶ã€‚

**åç«¯**

```python
# backend/routers/batch.py

@router.post("/api/batch-translate")
async def batch_translate(files: List[UploadFile]):
    results = []
    for file in files:
        content = await file.read()
        # å¤ç”¨ç°æœ‰ç¿»è¯‘é€»è¾‘
        doc_id = await create_translation_task(content.decode(), file.filename)
        results.append({"filename": file.filename, "docId": doc_id})
    return {"tasks": results}
```

**å‰ç«¯**

```tsx
// src/src/components/BatchUpload.tsx

export function BatchUpload() {
  const [files, setFiles] = useState<File[]>([]);
  
  const handleUpload = async () => {
    const formData = new FormData();
    files.forEach(f => formData.append('files', f));
    
    const res = await fetch('/api/batch-translate', {
      method: 'POST',
      body: formData
    });
    
    const { tasks } = await res.json();
    // ä¸ºæ¯ä¸ªä»»åŠ¡å»ºç«‹ WebSocket è¿æ¥...
  };
  
  return (
    <div>
      <input 
        type="file" 
        multiple 
        onChange={e => setFiles([...e.target.files])}
      />
      <button onClick={handleUpload}>æ‰¹é‡ç¿»è¯‘</button>
    </div>
  );
}
```

---

## ä¸‹ä¸€æ­¥

- ğŸš€ [éƒ¨ç½²æŒ‡å—](./08-éƒ¨ç½²æŒ‡å—.md) - ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²
- ğŸ“– [å¿«é€Ÿå…¥é—¨](./01-å¿«é€Ÿå…¥é—¨.md) - å›é¡¾åŸºç¡€
- ğŸ—ï¸ [ç³»ç»Ÿæ¶æ„](./02-ç³»ç»Ÿæ¶æ„.md) - ç†è§£æ•´ä½“è®¾è®¡
